{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importação dos pacotes\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, \\\n",
    "    accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Carregar os dados\n",
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "y_train = pd.read_pickle('y_train.pkl')['label']\n",
    "X_test  = pd.read_pickle('X_test.pkl')\n",
    "y_test  = pd.read_pickle('y_test.pkl')['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
      "       'WALKING_UPSTAIRS'],\n",
      "      dtype='object')\n",
      "(7352, 562) (7352,) (2947, 562) (2947,)\n"
     ]
    }
   ],
   "source": [
    "#%% Verificar as categorias das labels\n",
    "níveis = y_test.cat.categories\n",
    "print(níveis)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Definir o espaço de busca de hiperparâmetros para o LightGBM\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 500),  # Número de árvores\n",
    "    'max_depth': Integer(3, 15),       # Profundidade máxima das árvores\n",
    "    'learning_rate': Real(0.01, 0.3, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'num_leaves': Integer(20, 100),    # Número máximo de folhas\n",
    "    'min_child_samples': Integer(10, 100),  # Número mínimo de amostras por folha\n",
    "    'subsample': Real(0.5, 1.0),       # Subamostragem de dados\n",
    "    'colsample_bytree': Real(0.5, 1.0),  # Subamostragem de features\n",
    "    'reg_alpha': Real(0, 1),           # Regularização L1\n",
    "    'reg_lambda': Real(0, 1),          # Regularização L2\n",
    "    'boosting_type': Categorical(['gbdt', 'dart'])  # Tipo de boosting\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Configurar o modelo LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(random_state=2244000, verbose=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Configurar o Bayesian Search\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=5,  # Número de iterações\n",
    "    cv=2,       # Número de folds na validação cruzada\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usar todos os núcleos do processador\n",
    "    verbose=1,\n",
    "    random_state=2244000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Tempo de execução: 0 days 00:00:41.072518\n"
     ]
    }
   ],
   "source": [
    "#%% Executar o Bayesian Search\n",
    "tempo_ini = pd.Timestamp.now()  # Início do cronômetro\n",
    "bayes_search.fit(X_train, y_train)\n",
    "tempo_fim = pd.Timestamp.now()  # Fim do cronômetro\n",
    "print(f\"Tempo de execução: {tempo_fim - tempo_ini}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: OrderedDict([('boosting_type', 'gbdt'), ('colsample_bytree', 0.882556913536217), ('learning_rate', 0.013342604749226673), ('max_depth', 7), ('min_child_samples', 85), ('n_estimators', 355), ('num_leaves', 47), ('reg_alpha', 0.5039269732516942), ('reg_lambda', 0.9184079533796609), ('subsample', 0.9531043958851619)])\n"
     ]
    }
   ],
   "source": [
    "#%% Melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros:\", bayes_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Gerar as previsões do modelo\n",
    "pred = bayes_search.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusão:\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 429  61   0   0   1]\n",
      " [  0  29 503   0   0   0]\n",
      " [  0   0   0 490   2   4]\n",
      " [  0   0   0   8 381  31]\n",
      " [  0   0   0  31   6 434]]\n",
      "\n",
      "Relatório de Classificação:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       1.00      1.00      1.00       537\n",
      "           SITTING       0.94      0.87      0.90       491\n",
      "          STANDING       0.89      0.95      0.92       532\n",
      "           WALKING       0.93      0.99      0.96       496\n",
      "WALKING_DOWNSTAIRS       0.98      0.91      0.94       420\n",
      "  WALKING_UPSTAIRS       0.92      0.92      0.92       471\n",
      "\n",
      "          accuracy                           0.94      2947\n",
      "         macro avg       0.94      0.94      0.94      2947\n",
      "      weighted avg       0.94      0.94      0.94      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%% Gerar a matriz de confusão e estatísticas\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(cm)\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Gerar um DataFrame temporário para avaliar o modelo\n",
    "lgb_aval = pd.DataFrame({\n",
    "    'pred': pred,\n",
    "    'obs': y_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Função personalizada para summary (multiClassSummary equivalente)\n",
    "def multiClassSummary(df, levels):\n",
    "    report = classification_report(df['obs'], df['pred'], output_dict=True)\n",
    "    summary = {\n",
    "        'Accuracy': report['accuracy']\n",
    "    }\n",
    "    for level in levels:\n",
    "        summary[f'{level} Precision'] = report[level]['precision']\n",
    "        summary[f'{level} Recall'] = report[level]['recall']\n",
    "        summary[f'{level} F1-score'] = report[level]['f1-score']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de Avaliação:\n",
      "Accuracy                        0.941296\n",
      "LAYING Precision                1.000000\n",
      "LAYING Recall                   1.000000\n",
      "LAYING F1-score                 1.000000\n",
      "SITTING Precision               0.936681\n",
      "SITTING Recall                  0.873727\n",
      "SITTING F1-score                0.904110\n",
      "STANDING Precision              0.891844\n",
      "STANDING Recall                 0.945489\n",
      "STANDING F1-score               0.917883\n",
      "WALKING Precision               0.926276\n",
      "WALKING Recall                  0.987903\n",
      "WALKING F1-score                0.956098\n",
      "WALKING_DOWNSTAIRS Precision    0.979434\n",
      "WALKING_DOWNSTAIRS Recall       0.907143\n",
      "WALKING_DOWNSTAIRS F1-score     0.941904\n",
      "WALKING_UPSTAIRS Precision      0.923404\n",
      "WALKING_UPSTAIRS Recall         0.921444\n",
      "WALKING_UPSTAIRS F1-score       0.922423\n",
      "dtype: float64\n",
      "A acurácia na base de teste foi de: 94.13%\n"
     ]
    }
   ],
   "source": [
    "#%% Calcular métricas de avaliação\n",
    "metrics = pd.Series(multiClassSummary(lgb_aval, níveis))\n",
    "print(\"\\nMétricas de Avaliação:\")\n",
    "print(metrics)\n",
    "acc_teste = accuracy_score(y_test, lgb_aval.pred)\n",
    "print(f'A acurácia na base de teste foi de: {acc_teste:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
